{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon, MultiPoint\n",
    "from shapely.ops import unary_union\n",
    "from scipy.spatial import Voronoi\n",
    "from pyproj import Transformer\n",
    "import json\n",
    "from shapely.strtree import STRtree\n",
    "from collections import Counter\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_poi_points(poi_json_file):\n",
    "    with open(poi_json_file, 'r') as f:\n",
    "        poi_data = json.load(f)\n",
    "    \n",
    "    points = [Point(feature['geometry']['coordinates']) for feature in poi_data['features']]\n",
    "    return points\n",
    "\n",
    "def categorize_pois(poi_json_file, top_k=5):\n",
    "    with open(poi_json_file, 'r') as f:\n",
    "        poi_data = json.load(f)\n",
    "    \n",
    "    categories = {}\n",
    "    for feature in poi_data['features']:\n",
    "        try:\n",
    "            category = feature['properties']['categories']['primary']\n",
    "            point = Point(feature['geometry']['coordinates'])\n",
    "            if category not in categories:\n",
    "                categories[category] = []\n",
    "            categories[category].append(point)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Get top 5 categories\n",
    "    category_counts = {cat: len(pts) for cat, pts in categories.items()}\n",
    "    top_k_categories = [cat for cat, count in Counter(category_counts).most_common(top_k)]\n",
    "    \n",
    "    filtered_categories = {cat: pts for cat, pts in categories.items() if cat in top_k_categories}\n",
    "\n",
    "    return filtered_categories\n",
    "\n",
    "def build_convex_hull_boundary_from_poi(points):\n",
    "    points_gdf = gpd.GeoDataFrame(geometry=points, crs=\"EPSG:4326\")\n",
    "    polygon_boundary = points_gdf.unary_union.convex_hull\n",
    "    return polygon_boundary\n",
    "\n",
    "def buffer_polygon_boundary(polygon_boundary, buffer_distance=50):\n",
    "    buffered_polygon = polygon_boundary.buffer(buffer_distance)\n",
    "    return buffered_polygon\n",
    "\n",
    "def create_voronoi_torch(points, boundary, device=\"cpu\"):\n",
    "    if len(points) < 3:\n",
    "        return None\n",
    "    coords = torch.tensor([[p.x, p.y] for p in points], device=device)\n",
    "    coords_np = coords.cpu().numpy()\n",
    "    vor = Voronoi(coords_np)\n",
    "    \n",
    "    polygons = []\n",
    "    for region_idx in vor.regions:\n",
    "        if not region_idx or -1 in region_idx:\n",
    "            continue\n",
    "        polygon_points = [vor.vertices[i] for i in region_idx]\n",
    "        poly = Polygon(polygon_points)\n",
    "        clipped_poly = poly.intersection(boundary)\n",
    "        if not clipped_poly.is_empty:\n",
    "            polygons.append(clipped_poly)\n",
    "    return polygons\n",
    "\n",
    "def generate_points_from_voronoi_torch(voronoi_polygons_list, device=\"cpu\", distance_threshold=1e-15):\n",
    "    points = []\n",
    "    \n",
    "    valid_polygons = []\n",
    "    for poly in voronoi_polygons_list:\n",
    "        if isinstance(poly, (Polygon, MultiPolygon)):\n",
    "            if not poly.is_empty:\n",
    "                valid_polygons.append(poly)\n",
    "    \n",
    "    rtree = STRtree(valid_polygons)\n",
    "    \n",
    "    # Process each valid polygon\n",
    "    for poly in valid_polygons:\n",
    "        if isinstance(poly, Polygon):\n",
    "            centroid = torch.tensor([poly.centroid.x, poly.centroid.y], device=device)\n",
    "            distance_to_boundary = poly.boundary.distance(poly.centroid)\n",
    "            \n",
    "            if distance_to_boundary < float(distance_threshold):\n",
    "                # Intersection points: High-density regions\n",
    "                try:\n",
    "                    nearby_polygons = rtree.query(poly)\n",
    "                    for poly2 in nearby_polygons:\n",
    "                        if poly != poly2:\n",
    "                            intersection = poly.intersection(poly2)\n",
    "                            if isinstance(intersection, Point):\n",
    "                                points.append([intersection.x, intersection.y])\n",
    "                            elif isinstance(intersection, MultiPolygon):\n",
    "                                points.extend([[p.x, p.y] for p in intersection])\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                # Far-from-boundary: Centroid\n",
    "                points.append(centroid.cpu().numpy())\n",
    "    return points\n",
    "\n",
    "def merge_points(points_list):\n",
    "    valid_points = []\n",
    "    for p in points_list:\n",
    "        valid_points.append(Point(p[0], p[1]))\n",
    "    merged_geometry = unary_union(valid_points)\n",
    "    return merged_geometry\n",
    "\n",
    "def validate_points(points, poi_buffer, boundary):\n",
    "    validated_points = []\n",
    "\n",
    "    if isinstance(points, Point):\n",
    "        if poi_buffer.contains(points) and boundary.contains(points):\n",
    "            validated_points.append(points)\n",
    "    elif isinstance(points, MultiPoint):\n",
    "        for point in points.geoms:\n",
    "            if poi_buffer.contains(point) and boundary.contains(point):\n",
    "                validated_points.append(point)\n",
    "    return validated_points\n",
    "\n",
    "def create_output_schema(output_file, validated_points, transformer, eps1=0.01, eps2=0.01, eps3=0.03, min_samples1=3, min_samples2=4, min_samples3=6):\n",
    "    coordinates = [(poi.y, poi.x) for poi in validated_points]\n",
    "\n",
    "    # Apply DBSCAN\n",
    "    dbscan1 = DBSCAN(eps=eps1, min_samples=min_samples1)\n",
    "    labels1 = dbscan1.fit_predict(coordinates)\n",
    "    \n",
    "    # Prepare 2 lists for distinct types\n",
    "    first_list_unhandled = []\n",
    "    second_stage_clusters = []\n",
    "\n",
    "    # Prepare 2 lists for preliminary outputs between clustering\n",
    "    first_list_for_clustering = []\n",
    "    second_list_for_clustering = []\n",
    "    output_counter = 0\n",
    "\n",
    "    for idx, label1 in enumerate(labels1):\n",
    "        if label1 == -1:\n",
    "            first_list_for_clustering.append((idx, coordinates[idx], label1))\n",
    "        else:\n",
    "            second_list_for_clustering.append((idx, coordinates[idx], label1))\n",
    "    \n",
    "    first_stage_clusters = {}\n",
    "    for idx, point, label1 in first_list_for_clustering:\n",
    "        if label1 not in first_stage_clusters:\n",
    "            first_stage_clusters[label1] = []\n",
    "        first_stage_clusters[label1].append((idx, point))\n",
    "    \n",
    "    for label1, cluster_points in first_stage_clusters.items():\n",
    "        points_for_second_dbscan = [p[1] for p in cluster_points]\n",
    "        \n",
    "        if len(points_for_second_dbscan) > 1:\n",
    "            # Apply second DBSCAN\n",
    "            dbscan3 = DBSCAN(eps=eps3, min_samples=min_samples3)\n",
    "            labels3 = dbscan3.fit_predict(points_for_second_dbscan)\n",
    "            \n",
    "            unique_labels3 = set(labels3)\n",
    "            for label3 in unique_labels3:\n",
    "                cluster_subpoints = [points_for_second_dbscan[i] for i, lbl in enumerate(labels3) if lbl == label3]\n",
    "                \n",
    "                # Calculate centroid of the second-stage clusters\n",
    "                centroid_lat = sum([p[0] for p in cluster_subpoints]) / len(cluster_subpoints)\n",
    "                centroid_lon = sum([p[1] for p in cluster_subpoints]) / len(cluster_subpoints)\n",
    "                \n",
    "                x, y = transformer.transform(centroid_lat, centroid_lon)\n",
    "                \n",
    "                # Assign capacity based on whether it's merged or not\n",
    "                capacity = 2 if len(cluster_subpoints) > 1 else 0\n",
    "                \n",
    "                if capacity > 0:\n",
    "                    row = {\n",
    "                        'id': str(output_counter),\n",
    "                        'latitude': round(centroid_lat, 6),\n",
    "                        'longitude': round(centroid_lon, 6),\n",
    "                        'capacity': capacity,\n",
    "                        'geometry': Point(x, y)\n",
    "                        #'geometry': Point(centroid_lon, centroid_lat)\n",
    "                    }\n",
    "                    first_list_unhandled.append(row)\n",
    "                    output_counter += 1\n",
    "\n",
    "\n",
    "    first_stage_clusters = {}\n",
    "    for idx, point, label1 in second_list_for_clustering:\n",
    "        if label1 not in first_stage_clusters:\n",
    "            first_stage_clusters[label1] = []\n",
    "        first_stage_clusters[label1].append((idx, point))\n",
    "\n",
    "    for label1, cluster_points in first_stage_clusters.items():\n",
    "        points_for_second_dbscan = [p[1] for p in cluster_points]\n",
    "        \n",
    "        if len(points_for_second_dbscan) > 1:\n",
    "            # Apply second DBSCAN for tighter clustering\n",
    "            dbscan2 = DBSCAN(eps=eps2, min_samples=min_samples2)\n",
    "            labels2 = dbscan2.fit_predict(points_for_second_dbscan)\n",
    "            \n",
    "            unique_labels2 = set(labels2)\n",
    "            for label2 in unique_labels2:\n",
    "                cluster_subpoints = [points_for_second_dbscan[i] for i, lbl in enumerate(labels2) if lbl == label2]\n",
    "                \n",
    "                # Calculate centroid of the second-stage clusters\n",
    "                centroid_lat = sum([p[0] for p in cluster_subpoints]) / len(cluster_subpoints)\n",
    "                centroid_lon = sum([p[1] for p in cluster_subpoints]) / len(cluster_subpoints)\n",
    "                \n",
    "                x, y = transformer.transform(centroid_lat, centroid_lon)\n",
    "                \n",
    "                # Assign capacity based on whether it's merged or not\n",
    "                capacity = 5 if len(cluster_subpoints) > 1 else 4\n",
    "                \n",
    "                row = {\n",
    "                    'id': str(output_counter),\n",
    "                    'latitude': round(centroid_lat, 6),\n",
    "                    'longitude': round(centroid_lon, 6),\n",
    "                    'capacity': capacity,\n",
    "                    'geometry': Point(x, y)\n",
    "                    #'geometry': Point(centroid_lon, centroid_lat)\n",
    "                }\n",
    "                second_stage_clusters.append(row)\n",
    "                output_counter += 1\n",
    "        else:\n",
    "            # If the cluster has only one point, treat it as unmerged\n",
    "            lat, lon = points_for_second_dbscan[0]\n",
    "            x, y = transformer.transform(lat, lon)\n",
    "            row = {\n",
    "                'id': str(output_counter),\n",
    "                'latitude': round(lat, 6),\n",
    "                'longitude': round(lon, 6),\n",
    "                'capacity': 3,\n",
    "                'geometry': Point(x, y)\n",
    "                #'geometry': Point(lon, lat)\n",
    "            }\n",
    "            first_list_unhandled.append(row)\n",
    "            output_counter += 1\n",
    "\n",
    "    # Combine first and second stage outputs\n",
    "    final_output = first_list_unhandled + second_stage_clusters\n",
    "\n",
    "    df = gpd.GeoDataFrame(final_output)\n",
    "    df = df[['id', 'latitude', 'longitude', 'capacity', 'geometry']]\n",
    "\n",
    "    df.to_file(output_file, driver=\"GPKG\", index=False)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def main(poi_json_file, output_geopkg_file, top_k=5, buffer_distance=5, distance_threshold=1e-15, device=\"cpu\"):\n",
    "    # Load POI data\n",
    "    points = load_poi_points(poi_json_file)\n",
    "    polygon_boundary = build_convex_hull_boundary_from_poi(points)\n",
    "    buffered_polygon = buffer_polygon_boundary(polygon_boundary, buffer_distance)\n",
    "\n",
    "    # Categorize POIs by category\n",
    "    categories = categorize_pois(poi_json_file, top_k)\n",
    "\n",
    "    # For each category, generate a Voronoi diagram\n",
    "    merged_points = []\n",
    "    for category, category_points in categories.items():\n",
    "        voronoi_polygons = create_voronoi_torch(category_points, buffered_polygon, device=device)\n",
    "        voronoi_points = generate_points_from_voronoi_torch(voronoi_polygons, device=device, distance_threshold=distance_threshold)\n",
    "        merged_points.extend(voronoi_points)\n",
    "\n",
    "    # Merge and validate\n",
    "    merged_points = merge_points(merged_points)\n",
    "    poi_buffer = unary_union([p.buffer(buffer_distance) for p in points])\n",
    "    validated_points = validate_points(merged_points, poi_buffer, polygon_boundary)\n",
    "    \n",
    "    # Hierachical cluster and generate the output \n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)\n",
    "    create_output_schema(output_geopkg_file, validated_points, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_json_file = \"ge-package-tojson-out.json\"\n",
    "output_geopkg_file = \"ev_charging_stations_isi_i2.gpkg\"\n",
    "    \n",
    "main(poi_json_file, output_geopkg_file, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polySeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
